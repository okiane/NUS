{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to: /home/centos/BigData/02_cleaning\n"
     ]
    }
   ],
   "source": [
    "####----Check correct working directory----####\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "if(wd != '/home/centos/BigData/02_cleaning/'):\n",
    "    from os import chdir\n",
    "    chdir(\"/home/centos/BigData/02_cleaning/\")\n",
    "    print(\"Directory changed to: \" + wd)\n",
    "else:\n",
    "    print(\"Correct current directory: \" + wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 525\n"
     ]
    }
   ],
   "source": [
    "####----Import dictionary----####\n",
    "with open('dict/Overall_Master.txt', encoding = 'ISO-8859-1') as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "\n",
    "print(\"List length: \" +str(len(ingredient_set)))\n",
    "#print(ingredient_set)\n",
    "#ingredient_set = ingredient_set[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyArrow in /home/centos/anaconda3/lib/python3.7/site-packages (0.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in /home/centos/anaconda3/lib/python3.7/site-packages (from PyArrow) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/centos/anaconda3/lib/python3.7/site-packages (from PyArrow) (1.17.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark in /home/centos/.local/lib/python3.7/site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/centos/.local/lib/python3.7/site-packages (from pyspark) (0.10.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Connecting Spark\n",
      "\n",
      "Spark Context Created\n"
     ]
    }
   ],
   "source": [
    "####----Connect to Spark via PySpark----####\n",
    "%pip install PyArrow\n",
    "%pip install pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, explode, col, lit, monotonically_increasing_id, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, CountVectorizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import findspark\n",
    "print(\"Connecting Spark\")\n",
    "\n",
    "#sc.close()\n",
    "#sqlContext.stop()\n",
    "\n",
    "\n",
    "#Settings necessary for sandboxing otherwise Pyspark too slow on standalone\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "sqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"12000\")\n",
    "#improve toPandas() \n",
    "sqlContext.setConf(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "print(\"\\nSpark Context Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# print(\"Spark Session Created\")\n",
    "#Python Java issues: a drawback in using pyspark instead of Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 525\n"
     ]
    }
   ],
   "source": [
    "#Creation of filtering words\n",
    "with open(\"dict/Overall_Master.txt\") as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "print(\"List length: \" + str(len(ingredient_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- cook_time_minutes: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- error: boolean (nullable = true)\n",
      " |-- footnotes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- prep_time_minutes: long (nullable = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- time_scraped: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####----Load json into Spark for data cleaning----####\n",
    "\n",
    "recipe_src = 'allrecipes'\n",
    "\n",
    "read_path = \"/home/centos/BigData/01_source/\" + recipe_src + \"*.json\"\n",
    "#read_path = \"hdfs://localhost:9000/01_raw/allrecipes*.json\"\n",
    "\n",
    "recipes_df = sqlContext.read.json(read_path)\n",
    "recipes_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "|               title|         ingredients|         description|        instructions|           photo_url|                 url|rating_stars|review_count|total_time_minutes|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "|Family Classic Me...|[1 1/2 pounds ext...|Stuffing mix adds...|[Preheat oven to ...|http://images.med...|http://allrecipes...|         0.0|           0|                60|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|\n",
      "|            Basboosa|[1 1/2 cups semol...|This is a traditi...|[In a medium bowl...|http://images.med...|http://allrecipes...|        4.32|          16|                50|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####----Create Spark DataFrame----####\n",
    "recipes_df.createOrReplaceTempView(\"allrecipes\")\n",
    "\n",
    "#overall table\n",
    "recipes_table = sqlContext.sql(\"SELECT title, ingredients, description, instructions, photo_url, url, rating_stars, review_count, total_time_minutes  FROM allrecipes ORDER BY RAND() LIMIT 2000\")\n",
    "recipes_table.show(5)\n",
    "\n",
    "# recipes_pandas = recipes_table.toPandas()\n",
    "# recipes_pandas.to_csv(\"recipes_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'onion', 'corn', 'flour', 'sugar', '', 'salt', '', 'buttermilk', 'egg', 'cheese', 'corn', 'bell pepper', 'basil']\n",
      "525\n",
      "UDF Successful.\n"
     ]
    }
   ],
   "source": [
    "####----Extract ingredient----####\n",
    "import re\n",
    "\n",
    "##User-defined Functions\n",
    "def removeParenthesis(li):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = text.strip()\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "def extractIngredient(li, ingredient_set):\n",
    "    output = []\n",
    "    for item in li:\n",
    "        temp_output = []\n",
    "        for set_tracker in range(len(ingredient_set)):\n",
    "            check = bool(re.search(ingredient_set[set_tracker], item))\n",
    "            if (check == True):    \n",
    "                temp_output.append(ingredient_set[set_tracker])\n",
    "        if (len(temp_output) != 0):\n",
    "            temp_counter = 0\n",
    "            temp_tracker = 0\n",
    "            for temp in range(len(temp_output)):\n",
    "                count_temp = len(temp_output[temp])\n",
    "                if (count_temp > temp_counter):\n",
    "                    temp_tracker = temp\n",
    "                    temp_counter = count_temp\n",
    "            output.append(temp_output[temp_tracker])\n",
    "#             output.append(temp_output[[len(i) for i in temp_output].index(max([len(i) for i in temp_output]))])\n",
    "    return output\n",
    "\n",
    "sample = ['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil']\n",
    "\n",
    "test = extractIngredient(sample, ingredient_set)\n",
    "print(test)\n",
    "print(len(ingredient_set))\n",
    "\n",
    "##Create user defined function\n",
    "udf_removeParen = udf(removeParenthesis, ArrayType(StringType()))\n",
    "udf_extractIngredient = udf(lambda x: extractIngredient(x,ingredient_set), ArrayType(StringType()))\n",
    "udf_countList = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "print(\"UDF Successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defined Function - Feature Engineering \n",
    "#Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "#Safer to start off as non-vegetarian\n",
    "def detectVege(li, vegDetect_list):\n",
    "    label = 0\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in vegDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "#Safer to start off as positive allergy\n",
    "def detectNut(li):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (\"nut\" in text):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectDairy(li):\n",
    "    label = 1\n",
    "    dairy_list = [\"cheese\", \"milk\", \"yoghurt\", \"cream\"]\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (text in dairy_list):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectSeafood(li, seaDetect_list):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in seaDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "with open(\"dict/vegDetect.txt\") as veg_file:\n",
    "    vegDetect_list = veg_file.read().splitlines()\n",
    "\n",
    "with open(\"dict/seafoodDetect.txt\") as seafood_file:\n",
    "    seaDetect_list = seafood_file.read().splitlines()\n",
    "    \n",
    "#create user defined function\n",
    "udf_detectVege = udf(lambda x: detectVege(x, vegDetect_list), IntegerType())\n",
    "udf_detectNut = udf(detectNut, IntegerType())\n",
    "udf_detectDairy = udf(detectDairy, IntegerType())\n",
    "udf_detectSeafood = udf(lambda x: detectSeafood(x, seaDetect_list), IntegerType())\n",
    "\n",
    "#to stabilise schema with empty column\n",
    "# udf_empty = udf(lambda x: None, StringType())\n",
    "\n",
    "#create variable for time stamp\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191124-004049\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "|               title|         ingredients|         description|        instructions|           photo_url|                 url|rating_stars|review_count|total_time_minutes|            rm_paren|  ingredient_extract|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "|Family Classic Me...|[1 1/2 pounds ext...|Stuffing mix adds...|[Preheat oven to ...|http://images.med...|http://allrecipes...|         0.0|           0|                60|[1 1/2 pounds ext...|[round, tomato, ,...|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|\n",
      "|            Basboosa|[1 1/2 cups semol...|This is a traditi...|[In a medium bowl...|http://images.med...|http://allrecipes...|        4.32|          16|                50|[1 1/2 cups semol...|[flour, sugar, , ...|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "20191124-004205\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "####----Create ingredient table----####\n",
    "##Create ingredient table\n",
    "extracted_df = recipes_table.withColumn('rm_paren', udf_removeParen('ingredients')) \\\n",
    "                           .withColumn('ingredient_extract', udf_extractIngredient('rm_paren'))\n",
    "\n",
    "extracted_df.show(5)\n",
    "extracted_df.cache()\n",
    "\n",
    "extracted_df.storageLevel\n",
    "# extracted_df.unpersist()\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191124-004206\n",
      "20191124-005012\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "extracted_df.count()\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191124-005625\n",
      "20191124-005625\n",
      "+--------------------+----------+\n",
      "|              Recipe|Ingredient|\n",
      "+--------------------+----------+\n",
      "|Family Classic Me...|     round|\n",
      "|Family Classic Me...|    tomato|\n",
      "|Family Classic Me...|       egg|\n",
      "|Family Classic Me...|     onion|\n",
      "|Family Classic Me...|     thyme|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "20191124-005625\n",
      "\n",
      "Upload completed and file stored in hdfs://localhost:9000/02_store/Ing_allrecipes_20191124-005625.csv\n"
     ]
    }
   ],
   "source": [
    "#Create recipe ingredient graph to store in HDFS\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "ingredient_extract = extracted_df.select(\"title\", \"ingredient_extract\")\n",
    "\n",
    "ingredient_list = ingredient_extract.withColumn('exploded',explode('ingredient_extract')) \\\n",
    "                                   .select(col('title').alias('Recipe'),col('exploded').alias('Ingredient'))\\\n",
    "\n",
    "ingredient_list = ingredient_list.filter(ingredient_list.Ingredient != \"\")\n",
    "#                       .withColumn(\"Frequency\", lit(1))\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "ingredient_list.show(5)\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "\n",
    "\n",
    "export_filename_Ing = 'Ing_' + recipe_src + '_' + (time.strftime('%Y%m%d-%H%M%S')) + '.csv'\n",
    "\n",
    "time.strftime('%Y%m%d-%H%M%S') #Track processing time\n",
    "\n",
    "ingredient_list_pandas = ingredient_list.toPandas()\n",
    "\n",
    "ingredient_list_pandas.to_csv(export_filename_Ing)\n",
    "\n",
    "\n",
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_filename_Ing)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put\", \"-f\", export_filename_Ing, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191124-005630\n",
      "20191124-005630\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+----------------+---------+-------+-------+-------------+-------------------+\n",
      "|               title|         ingredients|         description|        instructions|           photo_url|                 url|rating_stars|review_count|total_time_minutes|            rm_paren|  ingredient_extract|vegetarian_label|nut_label|lactose|seafood|extract_count|          timestamp|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+----------------+---------+-------+-------+-------------+-------------------+\n",
      "|Family Classic Me...|[1 1/2 pounds ext...|Stuffing mix adds...|[Preheat oven to ...|http://images.med...|http://allrecipes...|         0.0|           0|                60|[1 1/2 pounds ext...|[round, tomato, ,...|               0|        0|      0|      1|            7|2019-11-24 00:40:49|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|               0|        0|      0|      1|            5|2019-11-24 00:40:49|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|               0|        0|      0|      1|            5|2019-11-24 00:40:49|\n",
      "|            Basboosa|[1 1/2 cups semol...|This is a traditi...|[In a medium bowl...|http://images.med...|http://allrecipes...|        4.32|          16|                50|[1 1/2 cups semol...|[flour, sugar, , ...|               0|        1|      0|      1|           10|2019-11-24 00:40:49|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|               0|        0|      0|      1|            5|2019-11-24 00:40:49|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+----------------+---------+-------+-------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "20191124-005631\n"
     ]
    }
   ],
   "source": [
    "####----User Defined Function - Feature Engineering----####\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "##Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "labelled_df = extracted_df.withColumn(\"vegetarian_label\", udf_detectVege(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"nut_label\", udf_detectNut(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"lactose\", udf_detectDairy(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"seafood\", udf_detectSeafood(\"ingredient_extract\")) \\\n",
    "                          .withColumn('extract_count', udf_countList('ingredient_extract'))\\\n",
    "\n",
    "labelled_df = labelled_df.withColumn('timestamp', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "\n",
    "# labelled_df = labelled_df.withColumn(\"rating_stars\", udf_empty(\"ingredient_extract\")) \\\n",
    "#                          .withColumn(\"review_count\", udf_empty(\"ingredient_extract\"))\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "labelled_df.show(5)\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- vegetarian_label: integer (nullable = true)\n",
      " |-- nut_label: integer (nullable = true)\n",
      " |-- lactose: integer (nullable = true)\n",
      " |-- seafood: integer (nullable = true)\n",
      " |-- extract_count: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['rm_paren', 'ingredient_extract']\n",
    "labelled_df = labelled_df.drop(*columns_to_drop)\n",
    "labelled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191124-005652\n",
      "20191124-005652\n",
      "20191124-005652\n",
      "\n",
      "Upload completed and file stored in hdfs://localhost:9000/02_store/Recipe_allrecipes_20191124-005652.csv\n"
     ]
    }
   ],
   "source": [
    "####----Export Spark Data Frame----####\n",
    "export_Recipe = 'Recipe_' + recipe_src + '_' + (time.strftime('%Y%m%d-%H%M%S')) + '.csv'\n",
    "\n",
    "#let's make our toPandas() faster\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "labelled_pandas = labelled_df.toPandas()\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "labelled_pandas.to_csv(export_Recipe)\n",
    "\n",
    "print(time.strftime('%Y%m%d-%H%M%S')) #Track processing time\n",
    "\n",
    "\n",
    "# graph_pandas = ingredient_graph.toPandas()\n",
    "# graph_pandas.to_csv(\"ingredient_graph.csv\")\n",
    "\n",
    "#write json\n",
    "# labelled_json = labelled_df.toJSON()\n",
    "# json_output = labelled_json.collect()\n",
    "\n",
    "# with open(\"label_table.json\", \"w\") as file:\n",
    "#     for j in json_output:\n",
    "#         json.dump(j, file)\n",
    "\n",
    "\n",
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_Recipe)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put\", \"-f\", export_Recipe, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(json_output[2])\n",
    "with open(\"all_label.json\", \"w\") as file:\n",
    "    json.dump(j, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload completed and file stored in hdfs://localhost:9000/02_store/Graph_allrecipes_20191124-005819.csv\n"
     ]
    }
   ],
   "source": [
    "#Create recipe-recipe graph\n",
    "time.strftime('%Y%m%d-%H%M%S') #Track processing time\n",
    "\n",
    "recipeGraph_json = ingredient_extract.toJSON()\n",
    "recipeGraph_json = recipeGraph_json.collect()\n",
    "\n",
    "recipeGraph_list = []\n",
    "for js in recipeGraph_json:\n",
    "    js_output = json.loads(js)\n",
    "    recipeGraph_list.append(js_output)\n",
    "\n",
    "recipeA = []\n",
    "recipeB = []\n",
    "commonIng = []\n",
    "for first in range(len(recipeGraph_list)):\n",
    "    counter = 1\n",
    "    recipe_a = recipeGraph_list[first][\"title\"]\n",
    "    ing_a = recipeGraph_list[first][\"ingredient_extract\"]\n",
    "    for second in range(counter, len(recipeGraph_list)): \n",
    "        recipe_b = recipeGraph_list[second][\"title\"]\n",
    "        ing_b = recipeGraph_list[second][\"ingredient_extract\"]\n",
    "        common = len(set(ing_a).intersection(ing_b))\n",
    "        recipeA.append(recipe_a)\n",
    "        recipeB.append(recipe_b)\n",
    "        commonIng.append(common)\n",
    "    counter = counter + 1\n",
    "\n",
    "recipeGraph_dict = {\"recipeA\":recipeA, \"recipeB\":recipeB, \"common_ing\":commonIng}\n",
    "sharedRecipe_pandas = pd.DataFrame(recipeGraph_dict)\n",
    "export_Graph = 'Graph_' + recipe_src + '_' + (time.strftime('%Y%m%d-%H%M%S')) + '.csv'\n",
    "\n",
    "time.strftime('%Y%m%d-%H%M%S') #Track processing time\n",
    "\n",
    "sharedRecipe_pandas.to_csv(export_Graph)\n",
    "\n",
    "\n",
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_Graph)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put\", \"-f\", export_Graph, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Similarity matrix (Too expensive)\n",
    "# tokenizer = Tokenizer(inputCol=\"Ingredient\", outputCol=\"Component\")\n",
    "# componentData = tokenizer.transform(cleanIngredient_table)\n",
    "# hashingTF = HashingTF(inputCol=\"Component\", outputCol=\"Ing_Component\")\n",
    "# featurizedData = hashingTF.transform(componentData)\n",
    "# idf = IDF(inputCol=\"Ing_Component\", outputCol=\"Ing_Feature\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# normalizer = Normalizer(inputCol=\"Ing_Feature\", outputCol=\"norm\")\n",
    "# similarity = normalizer.transform(rescaledData)\n",
    "# sim_matrix = similarity.select(\"ID\", \"title\", \"norm\")\n",
    "# sim_matrix.show()\n",
    "# sim_test = sim_matrix.rdd.map(lambda row:IndexedRow(row.ID, row.norm.toArray()))\n",
    "# type(sim_test)\n",
    "\n",
    "# mat = IndexedRowMatrix(similarity.select(\"title\", \"norm\")\\\n",
    "#                      .rdd.map(lambda row: IndexedRow(row.title, row.norm.toArray()))).toBlockMatrix()\n",
    "# dot = mat.multiply(mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "# data.alias(\"i\").join(data.alias(\"j\"), col(\"i.title\") < col(\"j.title\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.title\").alias(\"i\"), \n",
    "#         col(\"j.title\").alias(\"j\"), \n",
    "#         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# sim_mat = IndexedRowMatrix(sim_test).toBlockMatrix()\n",
    "# dot = sim_mat.multiply(sim_mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "# similarity.alias(\"i\").join(similarity.alias(\"j\"), col(\"i.ID\") < col(\"j.ID\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.ID\").alias(\"i\"), \n",
    "#         col(\"j.ID\").alias(\"j\"), \n",
    "#         dot_udf(\"i.ID\", \"j.ID\").alias(\"cosine\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"Ing_Str\", outputCol=\"Ing_Feature\", vocabSize=3, minDF=2.0)\n",
    "# model = cv.fit(cleanIngredient_table)\n",
    "# result = model.transform(cleanIngredient_table)\n",
    "# result.show(5)\n",
    "# test = result.toPandas()\n",
    "# test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
